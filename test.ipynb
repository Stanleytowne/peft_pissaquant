{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/data/mfx/huggingface/meta-llama/Llama-3.2-1B\", device_map='cuda:0')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/mfx/huggingface/meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Write me a poem about the sea. The mother\", return_tensors=\"pt\").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Write me a poem about the sea. The mother of all oceans, the sea is a place of mystery and wonder. It is a place of beauty and danger, a place of life and death. It is a place of love and hate, a place of peace and war. It is a place of hope and despair, a place of joy and sorrow. It is a place of life and death, a place of beauty and danger, a place of love and hate, a place of peace and war. It is a place of hope and despair, a place of joy and sorrow. It is a place of life and death, a place of beauty and danger, a place of love\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(r=32, target_modules=['q_proj'], pissaquant_config={'pissaquant_bits': 4}, init_lora_weights='PiSSAQuant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0192, device='cuda:0')\n",
      "tensor(0.0042, device='cuda:0')\n",
      "tensor(0.0041, device='cuda:0')\n",
      "tensor(0.0098, device='cuda:0')\n",
      "tensor(0.0082, device='cuda:0')\n",
      "tensor(0.0071, device='cuda:0')\n",
      "tensor(0.0128, device='cuda:0')\n",
      "tensor(0.0074, device='cuda:0')\n",
      "tensor(0.0057, device='cuda:0')\n",
      "tensor(0.0065, device='cuda:0')\n",
      "tensor(0.0043, device='cuda:0')\n",
      "tensor(0.0135, device='cuda:0')\n",
      "tensor(0.0038, device='cuda:0')\n",
      "tensor(0.0081, device='cuda:0')\n",
      "tensor(0.0052, device='cuda:0')\n",
      "tensor(0.0027, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([210.2891,  17.0796,  11.6353,   9.5772,   8.9909,   8.3452,   8.1227,\n",
       "          8.0238,   7.9361,   7.7079,   7.5841,   7.1897,   7.1564,   7.0057,\n",
       "          6.7993,   6.7189,   6.6833,   6.4882,   6.4230,   6.3611,   6.3275,\n",
       "          6.1197,   6.0687,   6.0429,   5.9617,   5.8813,   5.8114,   5.7684,\n",
       "          5.6735,   5.5499,   5.5069,   5.3306], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.base_model.model.model.layers[0].self_attn.q_proj.lora_S.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Write me a poem about the sea. The mother of all oceans, the sea is a place of mystery and wonder. It is a place of beauty and danger, a place of life and death. It is a place of love and hate, a place of peace and war. It is a place of hope and despair, a place of joy and sorrow. It is a place of life and death, a place of beauty and danger, a place of love and hate, a place of peace and war. It is a place of hope and despair, a place of joy and sorrow. It is a place of life and death, a place of beauty and danger, a place of love\n"
     ]
    }
   ],
   "source": [
    "output = peft_model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = peft_model.merge_and_unload()\n",
    "# output = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "# print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-01 00:22:15,071] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mfx/miniconda3/envs/pissaquant/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/data/mfx/miniconda3/envs/pissaquant/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/pissaquant/lib/libcufile.so: undefined reference to `dlvsym'\n",
      "/data/mfx/miniconda3/envs/pissaquant/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/pissaquant/lib/libcufile.so: undefined reference to `dlopen'\n",
      "/data/mfx/miniconda3/envs/pissaquant/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/pissaquant/lib/libcufile.so: undefined reference to `dlclose'\n",
      "/data/mfx/miniconda3/envs/pissaquant/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/pissaquant/lib/libcufile.so: undefined reference to `dlerror'\n",
      "/data/mfx/miniconda3/envs/pissaquant/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/pissaquant/lib/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('PiSSAQuant-Llama-2-7B-r32/tokenizer_config.json',\n",
       " 'PiSSAQuant-Llama-2-7B-r32/special_tokens_map.json',\n",
       " 'PiSSAQuant-Llama-2-7B-r32/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.peft_config['default'].init_lora_weights = \"PiSSAQuant_load\"\n",
    "peft_model.save_pretrained(\"PiSSAQuant-Llama-2-7B-r32/pissaquant_init\")\n",
    "model = peft_model.unload()\n",
    "model.save_pretrained(\"PiSSAQuant-Llama-2-7B-r32\")\n",
    "tokenizer.save_pretrained(\"PiSSAQuant-Llama-2-7B-r32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight': tensor([[ 0.0216,  0.0216,  0.0216,  ...,  0.0228,  0.0228,  0.0228],\n",
      "        [ 0.0282,  0.0282,  0.0282,  ..., -0.0144, -0.0144, -0.0144],\n",
      "        [ 0.0250,  0.0250,  0.0250,  ...,  0.0203,  0.0203,  0.0203],\n",
      "        ...,\n",
      "        [ 0.0080,  0.0080,  0.0080,  ...,  0.0529,  0.0529,  0.0529],\n",
      "        [-0.0093, -0.0093, -0.0093,  ...,  0.0196,  0.0196,  0.0196],\n",
      "        [ 0.0427,  0.0427,  0.0427,  ..., -0.0285, -0.0285, -0.0285]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight': tensor([[ 0.0074,  0.0028, -0.0009,  ...,  0.0062, -0.0042, -0.0014],\n",
      "        [ 0.0148, -0.0168, -0.0028,  ..., -0.0015,  0.0144,  0.0046],\n",
      "        [ 0.0176,  0.0014, -0.0191,  ...,  0.0121,  0.0201, -0.0006],\n",
      "        ...,\n",
      "        [ 0.0313, -0.0737,  0.0151,  ..., -0.0331, -0.0085,  0.0176],\n",
      "        [ 0.0117,  0.0025, -0.0145,  ...,  0.0161, -0.0042,  0.0005],\n",
      "        [ 0.0353, -0.0653,  0.0398,  ..., -0.0134, -0.0530, -0.0036]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.0.self_attn.q_proj.lora_S': tensor([210.2891,  17.0796,  11.6353,   9.5772,   8.9909,   8.3452,   8.1227,\n",
      "          8.0238,   7.9361,   7.7079,   7.5841,   7.1897,   7.1564,   7.0057,\n",
      "          6.7993,   6.7189,   6.6833,   6.4882,   6.4230,   6.3611,   6.3275,\n",
      "          6.1197,   6.0687,   6.0429,   5.9617,   5.8813,   5.8114,   5.7684,\n",
      "          5.6735,   5.5499,   5.5069,   5.3306], device='cuda:0'), 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight': tensor([[ 0.0218,  0.0218,  0.0218,  ...,  0.0219,  0.0219,  0.0219],\n",
      "        [-0.0061, -0.0061, -0.0061,  ..., -0.0056, -0.0056, -0.0056],\n",
      "        [-0.0016, -0.0016, -0.0016,  ..., -0.0014, -0.0014, -0.0014],\n",
      "        ...,\n",
      "        [ 0.0092,  0.0092,  0.0092,  ...,  0.0104,  0.0104,  0.0104],\n",
      "        [ 0.0035,  0.0035,  0.0035,  ...,  0.0176,  0.0176,  0.0176],\n",
      "        [-0.0388, -0.0388, -0.0388,  ...,  0.0028,  0.0028,  0.0028]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight': tensor([[ 0.0189, -0.0215,  0.0168,  ...,  0.0041,  0.0393, -0.0127],\n",
      "        [ 0.0176, -0.0109, -0.0208,  ...,  0.0269,  0.0297,  0.0345],\n",
      "        [ 0.0194, -0.0061,  0.0083,  ...,  0.0224,  0.0382,  0.0036],\n",
      "        ...,\n",
      "        [ 0.0235,  0.0016, -0.0182,  ...,  0.0185,  0.0317,  0.0020],\n",
      "        [ 0.0212, -0.0070, -0.0082,  ..., -0.0212, -0.0198,  0.0398],\n",
      "        [ 0.0213, -0.0050,  0.0174,  ..., -0.0118, -0.0225,  0.0254]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.1.self_attn.q_proj.lora_S': tensor([156.6839,  15.0581,   6.9580,   6.5129,   6.4654,   5.7881,   5.6451,\n",
      "          5.5341,   5.4547,   5.4360,   5.3729,   5.2373,   5.1461,   5.0699,\n",
      "          4.9097,   4.8683,   4.8472,   4.8161,   4.7652,   4.6666,   4.6636,\n",
      "          4.5968,   4.5859,   4.5408,   4.4555,   4.4210,   4.3444,   4.3321,\n",
      "          4.2078,   4.1963,   4.1137,   4.0202], device='cuda:0'), 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight': tensor([[ 0.0219,  0.0219,  0.0219,  ...,  0.0230,  0.0230,  0.0230],\n",
      "        [-0.0079, -0.0079, -0.0079,  ...,  0.1045,  0.1045,  0.1045],\n",
      "        [ 0.0023,  0.0023,  0.0023,  ...,  0.0225,  0.0225,  0.0225],\n",
      "        ...,\n",
      "        [-0.0433, -0.0433, -0.0433,  ..., -0.0045, -0.0045, -0.0045],\n",
      "        [ 0.0312,  0.0312,  0.0312,  ...,  0.0118,  0.0118,  0.0118],\n",
      "        [-0.0147, -0.0147, -0.0147,  ...,  0.0033,  0.0033,  0.0033]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight': tensor([[ 0.0139,  0.0156, -0.0062,  ..., -0.0061, -0.0035,  0.0042],\n",
      "        [ 0.0180,  0.0074, -0.0180,  ..., -0.0017, -0.0384, -0.0361],\n",
      "        [ 0.0146,  0.0077, -0.0052,  ...,  0.0084,  0.0068,  0.0072],\n",
      "        ...,\n",
      "        [ 0.0268,  0.0532,  0.0064,  ..., -0.0278,  0.0047, -0.0156],\n",
      "        [ 0.0264,  0.0252, -0.0162,  ...,  0.0008, -0.0145, -0.0039],\n",
      "        [ 0.0244, -0.0063, -0.0239,  ..., -0.0285,  0.0128,  0.0142]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.10.self_attn.q_proj.lora_S': tensor([146.4509,   6.1773,   5.7507,   5.4796,   5.3259,   5.1907,   5.1085,\n",
      "          5.0744,   5.0185,   4.8890,   4.8397,   4.7671,   4.6871,   4.6300,\n",
      "          4.6097,   4.5688,   4.5253,   4.4960,   4.4399,   4.3752,   4.3486,\n",
      "          4.2816,   4.2398,   4.2128,   4.1128,   4.0810,   4.0229,   3.9691,\n",
      "          3.9210,   3.8725,   3.8083,   3.7044], device='cuda:0'), 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight': tensor([[ 0.0219,  0.0219,  0.0219,  ...,  0.0237,  0.0237,  0.0237],\n",
      "        [-0.0163, -0.0163, -0.0163,  ...,  0.1046,  0.1046,  0.1046],\n",
      "        [ 0.0016,  0.0016,  0.0016,  ...,  0.0507,  0.0507,  0.0507],\n",
      "        ...,\n",
      "        [ 0.0132,  0.0132,  0.0132,  ..., -0.0036, -0.0036, -0.0036],\n",
      "        [-0.0088, -0.0088, -0.0088,  ..., -0.0009, -0.0009, -0.0009],\n",
      "        [-0.0117, -0.0117, -0.0117,  ..., -0.0010, -0.0010, -0.0010]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight': tensor([[ 0.0195,  0.0115,  0.0138,  ...,  0.0137,  0.0669, -0.0134],\n",
      "        [ 0.0188,  0.0379,  0.0137,  ...,  0.0032, -0.0219, -0.0118],\n",
      "        [ 0.0202,  0.0885,  0.0721,  ...,  0.0115, -0.0074, -0.0457],\n",
      "        ...,\n",
      "        [ 0.0226,  0.0004,  0.0106,  ...,  0.0276,  0.0039,  0.0206],\n",
      "        [ 0.0140, -0.0045, -0.0044,  ...,  0.0012, -0.0053, -0.0141],\n",
      "        [ 0.0218, -0.0052, -0.0051,  ...,  0.0065,  0.0281,  0.0085]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.11.self_attn.q_proj.lora_S': tensor([148.8290,   6.8034,   6.4800,   5.8131,   5.5536,   5.4464,   5.2925,\n",
      "          5.0282,   5.0063,   4.9130,   4.8318,   4.8048,   4.7454,   4.7098,\n",
      "          4.6430,   4.5746,   4.5711,   4.5120,   4.4920,   4.4546,   4.4296,\n",
      "          4.3992,   4.3429,   4.3011,   4.2664,   4.2107,   4.1822,   4.1437,\n",
      "          4.0441,   3.9885,   3.9539,   3.9418], device='cuda:0'), 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight': tensor([[ 2.1612e-02,  2.1612e-02,  2.1612e-02,  ...,  2.3440e-02,\n",
      "          2.3440e-02,  2.3440e-02],\n",
      "        [ 6.0007e-03,  6.0007e-03,  6.0007e-03,  ...,  8.3265e-02,\n",
      "          8.3265e-02,  8.3265e-02],\n",
      "        [ 3.7681e-03,  3.7681e-03,  3.7681e-03,  ..., -4.1360e-02,\n",
      "         -4.1360e-02, -4.1360e-02],\n",
      "        ...,\n",
      "        [ 1.9879e-02,  1.9879e-02,  1.9879e-02,  ..., -4.2208e-05,\n",
      "         -4.2218e-05, -4.2217e-05],\n",
      "        [-3.3760e-02, -3.3760e-02, -3.3760e-02,  ...,  4.3667e-04,\n",
      "          4.3666e-04,  4.3669e-04],\n",
      "        [-9.4019e-03, -9.4019e-03, -9.4019e-03,  ..., -6.8501e-04,\n",
      "         -6.8500e-04, -6.8501e-04]], device='cuda:0'), 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight': tensor([[ 0.0073,  0.0040, -0.0084,  ...,  0.0015, -0.0027,  0.0061],\n",
      "        [ 0.0056,  0.0029,  0.0074,  ...,  0.0110,  0.0027, -0.0078],\n",
      "        [ 0.0081,  0.0036,  0.0099,  ..., -0.0025,  0.0055, -0.0086],\n",
      "        ...,\n",
      "        [ 0.0270, -0.1094, -0.0561,  ...,  0.0141, -0.0193,  0.0310],\n",
      "        [ 0.0321,  0.0019, -0.0111,  ...,  0.0431,  0.0102,  0.0792],\n",
      "        [ 0.0266, -0.0679, -0.0256,  ..., -0.0238,  0.0210,  0.0071]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.12.self_attn.q_proj.lora_S': tensor([151.4434,   8.6395,   7.9874,   7.8258,   7.2210,   6.8637,   6.5813,\n",
      "          6.3264,   6.2574,   6.2248,   5.9476,   5.9294,   5.6454,   5.5748,\n",
      "          5.4115,   5.3734,   5.2643,   5.1872,   5.1058,   5.0705,   5.0365,\n",
      "          4.9505,   4.8202,   4.7682,   4.7268,   4.6992,   4.5281,   4.5032,\n",
      "          4.4569,   4.3574,   4.2949,   4.1256], device='cuda:0'), 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight': tensor([[ 0.0217,  0.0217,  0.0217,  ...,  0.0241,  0.0241,  0.0241],\n",
      "        [-0.0004, -0.0004, -0.0004,  ...,  0.1081,  0.1081,  0.1081],\n",
      "        [-0.0088, -0.0088, -0.0088,  ...,  0.0483,  0.0483,  0.0483],\n",
      "        ...,\n",
      "        [ 0.0311,  0.0311,  0.0311,  ...,  0.0042,  0.0042,  0.0042],\n",
      "        [ 0.0200,  0.0200,  0.0200,  ...,  0.0020,  0.0020,  0.0020],\n",
      "        [ 0.0158,  0.0158,  0.0158,  ..., -0.0019, -0.0019, -0.0019]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight': tensor([[ 0.0140,  0.0017, -0.0274,  ..., -0.0294, -0.0116,  0.0080],\n",
      "        [ 0.0103,  0.0017,  0.0043,  ...,  0.0056, -0.0133,  0.0083],\n",
      "        [ 0.0092,  0.0007, -0.0101,  ...,  0.0034, -0.0063,  0.0124],\n",
      "        ...,\n",
      "        [ 0.0308, -0.0824,  0.0767,  ...,  0.0248,  0.0188, -0.0138],\n",
      "        [ 0.0314, -0.0176,  0.0278,  ...,  0.0089,  0.0095,  0.0284],\n",
      "        [ 0.0322, -0.0585,  0.0280,  ..., -0.0076,  0.0428, -0.0389]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.13.self_attn.q_proj.lora_S': tensor([147.1042,   8.2210,   7.3040,   7.0195,   6.6646,   6.3691,   6.0424,\n",
      "          5.8502,   5.7238,   5.6809,   5.4074,   5.3489,   5.2379,   5.1749,\n",
      "          5.1366,   5.0672,   4.9578,   4.9150,   4.8473,   4.8228,   4.7874,\n",
      "          4.6722,   4.6601,   4.5964,   4.5180,   4.4999,   4.4860,   4.3876,\n",
      "          4.2740,   4.1834,   4.0846,   4.0114], device='cuda:0'), 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight': tensor([[ 2.0947e-02,  2.0947e-02,  2.0947e-02,  ...,  2.2630e-02,\n",
      "          2.2630e-02,  2.2630e-02],\n",
      "        [-1.6084e-02, -1.6084e-02, -1.6084e-02,  ..., -1.8612e-02,\n",
      "         -1.8612e-02, -1.8612e-02],\n",
      "        [-3.4804e-03, -3.4804e-03, -3.4804e-03,  ..., -7.2978e-02,\n",
      "         -7.2978e-02, -7.2978e-02],\n",
      "        ...,\n",
      "        [-7.1742e-02, -7.1742e-02, -7.1742e-02,  ...,  3.2719e-05,\n",
      "          3.2734e-05,  3.2723e-05],\n",
      "        [-2.4320e-02, -2.4320e-02, -2.4320e-02,  ..., -2.1130e-03,\n",
      "         -2.1130e-03, -2.1130e-03],\n",
      "        [ 1.8109e-02,  1.8109e-02,  1.8109e-02,  ..., -1.2805e-03,\n",
      "         -1.2804e-03, -1.2805e-03]], device='cuda:0'), 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight': tensor([[ 0.0098, -0.0095, -0.0178,  ...,  0.0071,  0.0088,  0.0179],\n",
      "        [ 0.0206, -0.0110, -0.0139,  ..., -0.0244,  0.0133,  0.0127],\n",
      "        [ 0.0137, -0.0057, -0.0427,  ..., -0.0100,  0.0100, -0.0033],\n",
      "        ...,\n",
      "        [ 0.0258,  0.0383,  0.0221,  ...,  0.0114, -0.0118,  0.0253],\n",
      "        [ 0.0231, -0.0013,  0.0038,  ...,  0.0138, -0.0094,  0.0131],\n",
      "        [ 0.0417,  0.0043, -0.0003,  ...,  0.0559,  0.0157,  0.0020]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.14.self_attn.q_proj.lora_S': tensor([153.7681,  11.3292,   9.3361,   9.0627,   8.5831,   7.9698,   7.4542,\n",
      "          7.0655,   6.8774,   6.8022,   6.5864,   6.4411,   6.3046,   6.0635,\n",
      "          5.9886,   5.9147,   5.6737,   5.4978,   5.4057,   5.2757,   5.1606,\n",
      "          5.0746,   4.9831,   4.8825,   4.7392,   4.7182,   4.6880,   4.5679,\n",
      "          4.5389,   4.4109,   4.2998,   4.0543], device='cuda:0'), 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight': tensor([[ 0.0208,  0.0208,  0.0208,  ...,  0.0234,  0.0234,  0.0234],\n",
      "        [-0.0119, -0.0119, -0.0119,  ..., -0.0617, -0.0617, -0.0617],\n",
      "        [ 0.0102,  0.0102,  0.0102,  ..., -0.0694, -0.0694, -0.0694],\n",
      "        ...,\n",
      "        [ 0.0578,  0.0578,  0.0578,  ..., -0.0030, -0.0030, -0.0030],\n",
      "        [-0.0726, -0.0726, -0.0726,  ..., -0.0066, -0.0066, -0.0066],\n",
      "        [-0.0221, -0.0221, -0.0221,  ...,  0.0007,  0.0007,  0.0007]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight': tensor([[ 0.0147, -0.0178,  0.0113,  ...,  0.0049, -0.0051, -0.0011],\n",
      "        [ 0.0128, -0.0082,  0.0056,  ..., -0.0126, -0.0029, -0.0178],\n",
      "        [ 0.0161, -0.0180,  0.0026,  ..., -0.0296,  0.0264, -0.0131],\n",
      "        ...,\n",
      "        [ 0.0196, -0.0039,  0.0065,  ..., -0.0130, -0.0248, -0.0200],\n",
      "        [ 0.0339,  0.0572, -0.0728,  ...,  0.0102,  0.0325, -0.0056],\n",
      "        [ 0.0342,  0.0242,  0.0101,  ..., -0.0434, -0.0618, -0.0440]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.15.self_attn.q_proj.lora_S': tensor([144.2398,  12.0248,   9.8170,   9.0459,   8.7242,   7.9974,   7.6662,\n",
      "          7.6281,   7.1474,   6.9677,   6.7746,   6.6750,   6.4662,   6.4002,\n",
      "          6.2316,   6.1096,   5.8849,   5.5765,   5.5404,   5.2648,   5.1865,\n",
      "          5.0902,   4.9133,   4.8186,   4.7299,   4.6160,   4.4477,   4.3692,\n",
      "          4.2436,   4.1690,   4.0017,   3.8651], device='cuda:0'), 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight': tensor([[ 0.0228,  0.0228,  0.0228,  ...,  0.0227,  0.0227,  0.0227],\n",
      "        [-0.0203, -0.0203, -0.0203,  ...,  0.0004,  0.0004,  0.0004],\n",
      "        [-0.0235, -0.0235, -0.0235,  ..., -0.0280, -0.0280, -0.0280],\n",
      "        ...,\n",
      "        [ 0.0128,  0.0128,  0.0128,  ..., -0.0011, -0.0011, -0.0011],\n",
      "        [ 0.0003,  0.0003,  0.0003,  ...,  0.0057,  0.0057,  0.0057],\n",
      "        [-0.0212, -0.0212, -0.0212,  ...,  0.0011,  0.0011,  0.0011]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight': tensor([[ 0.0166,  0.0065, -0.0164,  ..., -0.0184,  0.0005, -0.0091],\n",
      "        [ 0.0160,  0.0184, -0.0076,  ...,  0.0072, -0.0069,  0.0100],\n",
      "        [ 0.0177, -0.0064, -0.0459,  ..., -0.0162, -0.0132,  0.0056],\n",
      "        ...,\n",
      "        [ 0.0123, -0.0003, -0.0107,  ..., -0.0120,  0.0006,  0.0244],\n",
      "        [ 0.0149,  0.0026, -0.0028,  ...,  0.0064, -0.0023,  0.0043],\n",
      "        [ 0.0136, -0.0004, -0.0020,  ...,  0.0017,  0.0330,  0.0231]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.2.self_attn.q_proj.lora_S': tensor([158.7504,  11.2691,   7.9636,   7.6165,   7.1124,   7.0565,   6.9169,\n",
      "          6.6690,   6.5483,   6.4479,   6.3160,   6.1679,   6.0256,   5.9343,\n",
      "          5.6790,   5.6402,   5.5501,   5.4236,   5.3914,   5.2420,   5.1054,\n",
      "          5.0629,   4.9938,   4.8464,   4.7630,   4.7416,   4.7010,   4.6558,\n",
      "          4.4699,   4.3913,   4.3781,   4.1742], device='cuda:0'), 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight': tensor([[ 0.0220,  0.0220,  0.0220,  ...,  0.0227,  0.0227,  0.0227],\n",
      "        [-0.0134, -0.0134, -0.0134,  ...,  0.0592,  0.0592,  0.0592],\n",
      "        [-0.0040, -0.0040, -0.0040,  ...,  0.0441,  0.0441,  0.0441],\n",
      "        ...,\n",
      "        [ 0.0312,  0.0312,  0.0312,  ...,  0.0107,  0.0107,  0.0107],\n",
      "        [ 0.0252,  0.0252,  0.0252,  ...,  0.0039,  0.0039,  0.0039],\n",
      "        [-0.0067, -0.0067, -0.0067,  ...,  0.0032,  0.0032,  0.0032]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight': tensor([[ 7.9141e-03,  3.1009e-03, -2.3593e-02,  ..., -3.0831e-03,\n",
      "          1.3122e-03, -3.6120e-04],\n",
      "        [ 9.7696e-03,  3.0532e-03, -8.6681e-03,  ...,  1.1628e-02,\n",
      "         -1.7128e-02, -2.2755e-03],\n",
      "        [ 8.6161e-03,  6.6977e-03,  4.7691e-03,  ...,  2.0438e-02,\n",
      "         -1.2235e-02, -7.1744e-03],\n",
      "        ...,\n",
      "        [ 2.3407e-02,  5.7515e-02,  1.9762e-02,  ..., -1.2327e-02,\n",
      "          1.1818e-02,  3.0807e-05],\n",
      "        [ 1.7030e-02, -4.1684e-02,  9.0214e-03,  ...,  9.2108e-03,\n",
      "         -1.4731e-02, -1.1718e-02],\n",
      "        [ 2.7611e-02,  2.4927e-02, -2.2580e-02,  ..., -4.4633e-02,\n",
      "         -1.7391e-02, -2.5921e-02]], device='cuda:0'), 'base_model.model.model.layers.3.self_attn.q_proj.lora_S': tensor([150.0152,   6.1009,   5.9604,   5.7552,   5.6307,   5.5944,   5.5314,\n",
      "          5.2548,   5.1092,   5.0550,   5.0131,   5.0034,   4.8693,   4.8537,\n",
      "          4.7513,   4.6727,   4.6367,   4.6003,   4.5494,   4.5158,   4.4602,\n",
      "          4.4031,   4.3771,   4.3463,   4.3085,   4.2584,   4.1912,   4.1607,\n",
      "          4.0757,   4.0517,   3.9734,   3.9217], device='cuda:0'), 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight': tensor([[ 0.0223,  0.0223,  0.0223,  ...,  0.0226,  0.0226,  0.0226],\n",
      "        [ 0.0272,  0.0272,  0.0272,  ..., -0.0015, -0.0015, -0.0015],\n",
      "        [ 0.0055,  0.0055,  0.0055,  ..., -0.0533, -0.0533, -0.0533],\n",
      "        ...,\n",
      "        [-0.0010, -0.0010, -0.0010,  ..., -0.0075, -0.0075, -0.0075],\n",
      "        [ 0.0130,  0.0130,  0.0130,  ...,  0.0053,  0.0053,  0.0053],\n",
      "        [ 0.0383,  0.0383,  0.0383,  ..., -0.0035, -0.0035, -0.0035]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight': tensor([[ 0.0120,  0.0047,  0.0022,  ..., -0.0132,  0.0109,  0.0161],\n",
      "        [ 0.0082, -0.0027, -0.0032,  ...,  0.0082, -0.0022,  0.0193],\n",
      "        [ 0.0121, -0.0089, -0.0049,  ...,  0.0038, -0.0160,  0.0122],\n",
      "        ...,\n",
      "        [ 0.0221,  0.0182, -0.0329,  ..., -0.0103, -0.0262,  0.0014],\n",
      "        [ 0.0179, -0.0179, -0.0099,  ..., -0.0023, -0.0021,  0.0393],\n",
      "        [ 0.0172,  0.0017, -0.0261,  ..., -0.0370, -0.0512, -0.0007]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.4.self_attn.q_proj.lora_S': tensor([148.1435,   7.9657,   6.6738,   6.3678,   6.1011,   5.8622,   5.7685,\n",
      "          5.5579,   5.4028,   5.3020,   5.1850,   5.1125,   5.0102,   4.9912,\n",
      "          4.9148,   4.8315,   4.8176,   4.7580,   4.7028,   4.6719,   4.6153,\n",
      "          4.5463,   4.4004,   4.3911,   4.3694,   4.3167,   4.3132,   4.2069,\n",
      "          4.1192,   4.0374,   4.0099,   3.9099], device='cuda:0'), 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight': tensor([[ 0.0223,  0.0223,  0.0223,  ...,  0.0224,  0.0224,  0.0224],\n",
      "        [ 0.0050,  0.0050,  0.0050,  ..., -0.0008, -0.0008, -0.0008],\n",
      "        [-0.0130, -0.0130, -0.0130,  ...,  0.0149,  0.0149,  0.0149],\n",
      "        ...,\n",
      "        [ 0.0446,  0.0446,  0.0446,  ..., -0.0029, -0.0029, -0.0029],\n",
      "        [ 0.0166,  0.0166,  0.0166,  ...,  0.0239,  0.0239,  0.0239],\n",
      "        [-0.0194, -0.0194, -0.0194,  ...,  0.0013,  0.0013,  0.0013]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight': tensor([[ 5.7842e-03, -2.6957e-03, -9.9009e-04,  ..., -7.0834e-03,\n",
      "          1.3552e-02,  5.3532e-03],\n",
      "        [ 6.6087e-03, -3.0102e-03, -2.5458e-05,  ..., -1.6207e-03,\n",
      "          1.9112e-04, -9.0847e-03],\n",
      "        [ 5.9411e-03,  1.1180e-03, -1.2596e-03,  ...,  6.1091e-03,\n",
      "         -5.2787e-03, -5.1090e-03],\n",
      "        ...,\n",
      "        [ 2.5174e-02, -2.5084e-02, -4.1986e-03,  ..., -6.2525e-02,\n",
      "          4.7842e-03, -3.9776e-02],\n",
      "        [ 2.6632e-02,  1.5861e-02, -6.2763e-03,  ...,  2.3412e-02,\n",
      "         -3.5332e-02, -2.2881e-02],\n",
      "        [ 2.6009e-02, -1.2978e-02, -4.2488e-03,  ..., -1.5744e-02,\n",
      "          1.8530e-02,  1.7452e-02]], device='cuda:0'), 'base_model.model.model.layers.5.self_attn.q_proj.lora_S': tensor([145.9006,   9.6427,   8.5216,   7.7271,   7.6137,   7.1232,   6.8203,\n",
      "          6.2429,   6.0166,   5.7450,   5.5751,   5.4660,   5.3229,   5.2778,\n",
      "          5.1041,   5.0230,   4.8214,   4.7013,   4.5951,   4.5477,   4.4707,\n",
      "          4.4425,   4.4243,   4.3734,   4.2252,   4.1659,   4.0906,   4.0591,\n",
      "          4.0248,   3.9393,   3.9148,   3.8307], device='cuda:0'), 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight': tensor([[ 0.0221,  0.0221,  0.0221,  ...,  0.0220,  0.0220,  0.0220],\n",
      "        [-0.0016, -0.0016, -0.0016,  ...,  0.0124,  0.0124,  0.0124],\n",
      "        [-0.0031, -0.0031, -0.0031,  ...,  0.0172,  0.0172,  0.0172],\n",
      "        ...,\n",
      "        [-0.0270, -0.0270, -0.0270,  ..., -0.0024, -0.0024, -0.0024],\n",
      "        [-0.0085, -0.0085, -0.0085,  ...,  0.0170,  0.0170,  0.0170],\n",
      "        [ 0.0146,  0.0146,  0.0146,  ..., -0.0178, -0.0178, -0.0178]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight': tensor([[ 0.0194, -0.0366,  0.0135,  ...,  0.0053, -0.0083,  0.0234],\n",
      "        [ 0.0155, -0.0254,  0.0042,  ...,  0.0119,  0.0197,  0.0065],\n",
      "        [ 0.0235, -0.0210,  0.0583,  ...,  0.0365,  0.0405, -0.0122],\n",
      "        ...,\n",
      "        [ 0.0311, -0.0231, -0.0220,  ...,  0.0364,  0.0115,  0.0416],\n",
      "        [ 0.0292,  0.0066,  0.0288,  ..., -0.0180,  0.0082,  0.0070],\n",
      "        [ 0.0324,  0.0116, -0.0346,  ...,  0.0112, -0.0121, -0.0255]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.6.self_attn.q_proj.lora_S': tensor([134.6345,   5.0935,   4.8089,   4.7176,   4.6095,   4.5786,   4.5319,\n",
      "          4.4906,   4.4451,   4.4094,   4.3527,   4.3264,   4.2915,   4.2108,\n",
      "          4.2046,   4.1738,   4.1450,   4.0717,   4.0462,   4.0298,   3.9446,\n",
      "          3.9359,   3.8985,   3.8666,   3.8115,   3.7816,   3.7651,   3.7302,\n",
      "          3.6843,   3.6266,   3.5814,   3.5245], device='cuda:0'), 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight': tensor([[ 0.0220,  0.0220,  0.0220,  ...,  0.0223,  0.0223,  0.0223],\n",
      "        [ 0.0011,  0.0011,  0.0011,  ..., -0.0132, -0.0132, -0.0132],\n",
      "        [-0.0114, -0.0114, -0.0114,  ..., -0.0113, -0.0113, -0.0113],\n",
      "        ...,\n",
      "        [ 0.0123,  0.0123,  0.0123,  ..., -0.0192, -0.0192, -0.0192],\n",
      "        [-0.0038, -0.0038, -0.0038,  ..., -0.0012, -0.0012, -0.0012],\n",
      "        [-0.0047, -0.0047, -0.0047,  ..., -0.0074, -0.0074, -0.0074]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight': tensor([[ 0.0193, -0.0215,  0.0355,  ...,  0.0017, -0.0233, -0.0061],\n",
      "        [ 0.0164,  0.0081,  0.0212,  ...,  0.0180, -0.0186, -0.0301],\n",
      "        [ 0.0223,  0.0761, -0.0104,  ..., -0.0187,  0.0203, -0.0037],\n",
      "        ...,\n",
      "        [ 0.0107,  0.0055, -0.0173,  ...,  0.0071, -0.0018,  0.0081],\n",
      "        [ 0.0158, -0.0137,  0.0097,  ...,  0.0068, -0.0020,  0.0096],\n",
      "        [ 0.0147, -0.0040,  0.0070,  ..., -0.0032,  0.0050,  0.0235]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.7.self_attn.q_proj.lora_S': tensor([143.3856,   5.9434,   5.1761,   5.0098,   4.8959,   4.8598,   4.8095,\n",
      "          4.7662,   4.7118,   4.6517,   4.6350,   4.5614,   4.5102,   4.4423,\n",
      "          4.3963,   4.3198,   4.3064,   4.2395,   4.2042,   4.1590,   4.1308,\n",
      "          4.0978,   4.0237,   3.9682,   3.9125,   3.9041,   3.8533,   3.8322,\n",
      "          3.8108,   3.7478,   3.7142,   3.6375], device='cuda:0'), 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight': tensor([[ 0.0223,  0.0223,  0.0223,  ...,  0.0222,  0.0222,  0.0222],\n",
      "        [-0.0216, -0.0216, -0.0216,  ...,  0.0148,  0.0148,  0.0148],\n",
      "        [-0.0021, -0.0021, -0.0021,  ..., -0.0180, -0.0180, -0.0180],\n",
      "        ...,\n",
      "        [-0.0049, -0.0049, -0.0049,  ...,  0.0043,  0.0043,  0.0043],\n",
      "        [ 0.0197,  0.0197,  0.0197,  ..., -0.0279, -0.0279, -0.0279],\n",
      "        [ 0.0033,  0.0033,  0.0033,  ..., -0.0223, -0.0223, -0.0223]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight': tensor([[ 0.0130,  0.0144,  0.0058,  ..., -0.0098,  0.0061, -0.0109],\n",
      "        [ 0.0151, -0.0058, -0.0060,  ..., -0.0121, -0.0261, -0.0141],\n",
      "        [ 0.0149,  0.0069,  0.0079,  ...,  0.0200, -0.0025, -0.0193],\n",
      "        ...,\n",
      "        [ 0.0312,  0.0063, -0.0048,  ...,  0.0114,  0.0017, -0.0196],\n",
      "        [ 0.0302,  0.0085, -0.0234,  ..., -0.0092, -0.0657, -0.0283],\n",
      "        [ 0.0287,  0.0276,  0.0192,  ..., -0.0013, -0.0455,  0.0321]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.8.self_attn.q_proj.lora_S': tensor([145.1227,   9.8847,   7.0492,   6.7345,   6.2397,   6.1415,   5.9351,\n",
      "          5.7393,   5.6748,   5.5236,   5.3222,   5.1013,   5.0193,   4.8928,\n",
      "          4.8167,   4.6766,   4.6532,   4.5544,   4.5313,   4.4710,   4.4038,\n",
      "          4.3099,   4.2314,   4.2070,   4.1596,   4.1066,   4.0607,   4.0418,\n",
      "          3.9820,   3.8815,   3.8246,   3.7085], device='cuda:0'), 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight': tensor([[ 0.0220,  0.0220,  0.0220,  ...,  0.0228,  0.0228,  0.0228],\n",
      "        [-0.0072, -0.0072, -0.0072,  ..., -0.0529, -0.0529, -0.0529],\n",
      "        [ 0.0114,  0.0114,  0.0114,  ..., -0.0144, -0.0144, -0.0144],\n",
      "        ...,\n",
      "        [ 0.0026,  0.0026,  0.0026,  ...,  0.0044,  0.0044,  0.0044],\n",
      "        [ 0.0125,  0.0125,  0.0125,  ...,  0.0020,  0.0020,  0.0020],\n",
      "        [ 0.0025,  0.0025,  0.0025,  ..., -0.0024, -0.0024, -0.0024]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight': tensor([[ 0.0084, -0.0118, -0.0204,  ..., -0.0034, -0.0051, -0.0130],\n",
      "        [ 0.0126, -0.0244, -0.0147,  ...,  0.0069,  0.0047, -0.0296],\n",
      "        [ 0.0106, -0.0041, -0.0148,  ..., -0.0031,  0.0152, -0.0048],\n",
      "        ...,\n",
      "        [ 0.0291, -0.0033,  0.0496,  ..., -0.0069,  0.0124,  0.0002],\n",
      "        [ 0.0246, -0.0178,  0.0069,  ...,  0.0307, -0.0086,  0.0156],\n",
      "        [ 0.0244,  0.0194,  0.0163,  ..., -0.0295,  0.0034, -0.0329]],\n",
      "       device='cuda:0'), 'base_model.model.model.layers.9.self_attn.q_proj.lora_S': tensor([161.3810,   6.3512,   6.0557,   5.9572,   5.8580,   5.7275,   5.6351,\n",
      "          5.5449,   5.4433,   5.3501,   5.3016,   5.2410,   5.1605,   5.0980,\n",
      "          4.9894,   4.9857,   4.9468,   4.8813,   4.8711,   4.8105,   4.7184,\n",
      "          4.6236,   4.5814,   4.5359,   4.4486,   4.4058,   4.3633,   4.3161,\n",
      "          4.2865,   4.2644,   4.2216,   4.1519], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"PiSSAQuant-Llama-2-7B-r32\", device_map='cuda:0')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PiSSAQuant-Llama-2-7B-r32\")\n",
    "from peft import PeftModel\n",
    "peft_model = PeftModel.from_pretrained(model, \"PiSSAQuant-Llama-2-7B-r32/pissaquant_init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Write me a poem about the sea. The mother of all oceans, the sea is a place of mystery and wonder. It is a place of beauty and danger, a place of life and death. It is a place of love and hate, a place of peace and war. It is a place of hope and despair, a place of joy and sorrow. It is a place of life and death, a place of beauty and danger, a place of love and hate, a place of peace and war. It is a place of hope and despair, a place of joy and sorrow. It is a place of life and death, a place of beauty and danger, a place of love\n"
     ]
    }
   ],
   "source": [
    "output = peft_model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pissaquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
