{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/data/mfx/huggingface/meta-llama/Llama-3.2-1B\", device_map='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/mfx/huggingface/meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Here I'll write a poem about the sea.\", return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Here I'll write a poem about the sea. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0183,  0.0071,  0.0219,  ..., -0.0070, -0.0089,  0.0149],\n",
       "        [ 0.0112,  0.0593,  0.0630,  ..., -0.0334, -0.0148,  0.0058],\n",
       "        [ 0.0182,  0.0141,  0.0361,  ..., -0.0432, -0.0388, -0.0233],\n",
       "        ...,\n",
       "        [ 0.0305,  0.0289,  0.0801,  ..., -0.0767, -0.0311, -0.0334],\n",
       "        [ 0.0242, -0.0325,  0.0369,  ..., -0.0123, -0.0269, -0.0151],\n",
       "        [-0.0264, -0.0498, -0.0210,  ...,  0.0601,  0.0130, -0.0007]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(r=32, target_modules=['q_proj'], pissaquant_config={'pissaquant_bits': 8, 'apply_quantization': True}, init_lora_weights='PiSSAQuant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config.pissaquant_config['pissaquant_bits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1512,  0.1512,  0.1512,  ...,  0.0600,  0.0600,  0.0600],\n",
       "        [-0.0029, -0.0029, -0.0029,  ...,  0.1133,  0.1133,  0.1133],\n",
       "        [-0.0241, -0.0241, -0.0241,  ...,  0.0592,  0.0592,  0.0592],\n",
       "        ...,\n",
       "        [ 0.1102,  0.1102,  0.1102,  ...,  0.0794,  0.0794,  0.0794],\n",
       "        [-0.0198, -0.0198, -0.0198,  ..., -0.0679, -0.0679, -0.0679],\n",
       "        [ 0.0415,  0.0415,  0.0415,  ...,  0.1469,  0.1469,  0.1469]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.base_model.model.model.layers[0].self_attn.q_proj.lora_A['default'].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Here I'll write a poem about the sea. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of\n"
     ]
    }
   ],
   "source": [
    "output = peft_model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = peft_model.merge_and_unload()\n",
    "# output = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "# print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0184,  0.0071,  0.0218,  ..., -0.0069, -0.0089,  0.0148],\n",
       "        [ 0.0111,  0.0598,  0.0630,  ..., -0.0332, -0.0146,  0.0057],\n",
       "        [ 0.0183,  0.0143,  0.0359,  ..., -0.0430, -0.0386, -0.0234],\n",
       "        ...,\n",
       "        [ 0.0306,  0.0289,  0.0798,  ..., -0.0765, -0.0307, -0.0332],\n",
       "        [ 0.0243, -0.0324,  0.0368,  ..., -0.0124, -0.0269, -0.0150],\n",
       "        [-0.0260, -0.0498, -0.0210,  ...,  0.0595,  0.0132, -0.0009]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-15 14:07:57,128] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mfx/miniconda3/envs/pissaquant/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/data/mfx/miniconda3/envs/pissaquant/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/pissaquant/lib/libcufile.so: undefined reference to `dlvsym'\n",
      "/data/mfx/miniconda3/envs/pissaquant/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/pissaquant/lib/libcufile.so: undefined reference to `dlopen'\n",
      "/data/mfx/miniconda3/envs/pissaquant/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/pissaquant/lib/libcufile.so: undefined reference to `dlclose'\n",
      "/data/mfx/miniconda3/envs/pissaquant/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/pissaquant/lib/libcufile.so: undefined reference to `dlerror'\n",
      "/data/mfx/miniconda3/envs/pissaquant/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/pissaquant/lib/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('PiSSAQuant-Llama-3.2-1B/tokenizer_config.json',\n",
       " 'PiSSAQuant-Llama-3.2-1B/special_tokens_map.json',\n",
       " 'PiSSAQuant-Llama-3.2-1B/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.peft_config['default'].init_lora_weights = \"PiSSAQuant_load\"\n",
    "peft_model.save_pretrained(\"PiSSAQuant-Llama-3.2-1B/pissaquant_init\")\n",
    "model = peft_model.unload()\n",
    "model.save_pretrained(\"PiSSAQuant-Llama-3.2-1B\")\n",
    "tokenizer.save_pretrained(\"PiSSAQuant-Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"PiSSAQuant-Llama-3.2-1B\", device_map='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PiSSAQuant-Llama-3.2-1B\")\n",
    "from peft import PeftModel\n",
    "peft_model = PeftModel.from_pretrained(model, \"PiSSAQuant-Llama-3.2-1B/pissaquant_init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Here I'll write a poem about the sea. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of\n"
     ]
    }
   ],
   "source": [
    "output = peft_model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1512,  0.1512,  0.1512,  ...,  0.0600,  0.0600,  0.0600],\n",
       "        [-0.0029, -0.0029, -0.0029,  ...,  0.1133,  0.1133,  0.1133],\n",
       "        [-0.0241, -0.0241, -0.0241,  ...,  0.0592,  0.0592,  0.0592],\n",
       "        ...,\n",
       "        [ 0.1102,  0.1102,  0.1102,  ...,  0.0794,  0.0794,  0.0794],\n",
       "        [-0.0198, -0.0198, -0.0198,  ..., -0.0679, -0.0679, -0.0679],\n",
       "        [ 0.0415,  0.0415,  0.0415,  ...,  0.1469,  0.1469,  0.1469]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.base_model.model.model.layers[0].self_attn.q_proj.lora_A['default'].weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pissaquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
