{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/data/mfx/huggingface/meta-llama/Llama-3.2-1B\", device_map='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/mfx/huggingface/meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Here I'll write a poem about the sea.\", return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Here I'll write a poem about the sea. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0183,  0.0071,  0.0219,  ..., -0.0070, -0.0089,  0.0149],\n",
       "        [ 0.0112,  0.0593,  0.0630,  ..., -0.0334, -0.0148,  0.0058],\n",
       "        [ 0.0182,  0.0141,  0.0361,  ..., -0.0432, -0.0388, -0.0233],\n",
       "        ...,\n",
       "        [ 0.0305,  0.0289,  0.0801,  ..., -0.0767, -0.0311, -0.0334],\n",
       "        [ 0.0242, -0.0325,  0.0369,  ..., -0.0123, -0.0269, -0.0151],\n",
       "        [-0.0264, -0.0498, -0.0210,  ...,  0.0601,  0.0130, -0.0007]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(r=32, target_modules=['q_proj'], pissaquant_config={'pissaquant_bits': 4, 'apply_quantization': False}, init_lora_weights='PiSSAQuant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=32, target_modules={'q_proj'}, exclude_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights='PiSSAQuant', layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, pissaquant_config={'pissaquant_bits': 4, 'apply_quantization': False}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([210.2891,  17.0796,  11.6353,   9.5772,   8.9909,   8.3452,   8.1227,\n",
       "          8.0238,   7.9361,   7.7079,   7.5841,   7.1897,   7.1564,   7.0057,\n",
       "          6.7993,   6.7189,   6.6833,   6.4882,   6.4230,   6.3611,   6.3275,\n",
       "          6.1197,   6.0687,   6.0429,   5.9617,   5.8813,   5.8114,   5.7684,\n",
       "          5.6735,   5.5499,   5.5069,   5.3306], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.base_model.model.model.layers[0].self_attn.q_proj.lora_S.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/pissaquant/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Here I'll write a poem about the sea. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of a haiku. I'll write it in the form of\n"
     ]
    }
   ],
   "source": [
    "output = peft_model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = peft_model.merge_and_unload()\n",
    "# output = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "# print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0183,  0.0071,  0.0219,  ..., -0.0070, -0.0089,  0.0149],\n",
       "        [ 0.0112,  0.0593,  0.0630,  ..., -0.0334, -0.0148,  0.0058],\n",
       "        [ 0.0182,  0.0141,  0.0361,  ..., -0.0432, -0.0388, -0.0233],\n",
       "        ...,\n",
       "        [ 0.0305,  0.0289,  0.0801,  ..., -0.0767, -0.0311, -0.0334],\n",
       "        [ 0.0242, -0.0325,  0.0369,  ..., -0.0123, -0.0269, -0.0151],\n",
       "        [-0.0264, -0.0498, -0.0210,  ...,  0.0601,  0.0130, -0.0007]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.peft_config['default'].init_lora_weights = \"PiSSAQuant_load\"\n",
    "peft_model.save_pretrained(\"PiSSAQuant-Llama-3.2-1B/pissaquant_init\")\n",
    "model = peft_model.unload()\n",
    "model.save_pretrained(\"PiSSAQuant-Llama-3.2-1B\")\n",
    "tokenizer.save_pretrained(\"PiSSAQuant-Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"PiSSAQuant-Llama-3.2-1B\", device_map='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PiSSAQuant-Llama-3.2-1B\")\n",
    "from peft import PeftModel\n",
    "peft_model = PeftModel.from_pretrained(model, \"PiSSAQuant-Llama-3.2-1B/pissaquant_init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = peft_model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pissaquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
